{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644a401-7a9a-424b-b487-de987b2d3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Repository\\Book\\scripts\\onedrive\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "Bot has been started!!!\n",
      "Total messages: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The telegram bot related code is taken from https://github.com/cmd410/OrigamiBot\n",
    "and then modified with our LLM bot to have conversation with users\n",
    "\"\"\"\n",
    "\n",
    "# import packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "# Below will use HuggingFace - sentence-transformers\n",
    "# https://huggingface.co/sentence-transformers\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define directories\n",
    "pdf_file_dir_path = \"custom_data_chatbot/pdfs\"\n",
    "model_path = \"custom_data_chatbot/models\"\n",
    "\n",
    "MAX_MESSAGE_LENGTH = 4095  # Maximum length for a Telegram message\n",
    "\n",
    "def split_message(message):\n",
    "    \"\"\"Split a message into chunks of maximum length.\"\"\"\n",
    "    return [message[i:i+MAX_MESSAGE_LENGTH] for i in range(0, len(message), MAX_MESSAGE_LENGTH)]\n",
    "\n",
    "# Load  ................................................................................................................\n",
    "# Load data from PDF file.\n",
    "loader = DirectoryLoader(pdf_file_dir_path)\n",
    "\n",
    "# convert docs in to small chunks for better management\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# load data from pdf and create chunks for better management\n",
    "pages = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Defining global settings for easy and fast work\n",
    "\n",
    "# load text embedding model from HuggingFaceHub to generate vector embeddings ..........................................\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
    "    cache_folder=model_path,\n",
    "    # cpu because on AWS we are not using GPU\n",
    "    model_kwargs={\n",
    "        \"device\": \"cpu\",\n",
    "    },  # make it to \"cuda\" in case of GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    "    multi_process=True,\n",
    ")\n",
    "\n",
    "chroma_db = Chroma.from_documents(pages, embed_model, persist_directory=model_path)\n",
    "\n",
    "# Retrieve .............................................................................................................\n",
    "# define retriever to retrieve Question related Docs\n",
    "retriever = chroma_db.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum MArginal Relevance\n",
    "    search_kwargs={\"k\": 1},  # max relevan docs to retrieve\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir=model_path)\n",
    "\n",
    "# Define pipeline ......................................................................................................\n",
    "text_generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    token=\"PUT_HERE_HUGGINGFACEHUB_API_TOKEN\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # make it \"auto\" for auto selection between GPU and CPU, -1 for CPU, 0 for GPU\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,  # generate token sequences of 1024 including input and output token sequences\n",
    ")\n",
    "\n",
    "ms_dialo_gpt_hf = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "retrievalQA = RetrievalQA.from_llm(\n",
    "    llm=ms_dialo_gpt_hf,\n",
    "    retriever=retriever,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"{context}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# telegram related stuff -----------------------------------------------------------------------------------------------\n",
    "class BotsCommands:\n",
    "    \"\"\"\n",
    "    This are the commands which you can use in chat like..........\n",
    "    /start will start the conversation\n",
    "    /echo will echo the message\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bot: Bot):  # Can initialize however you like\n",
    "        self.bot = bot\n",
    "\n",
    "    def start(self, message):  # /start command\n",
    "        self.bot.send_message(message.chat.id, \"Hello user!\\nThis is an example bot.\")\n",
    "\n",
    "    def echo(self, message, value: str):  # /echo [value: str] command\n",
    "        self.bot.send_message(message.chat.id, value)\n",
    "\n",
    "    def _not_a_command(self):  # This method not considered a command\n",
    "        print(\"I am not a command\")\n",
    "\n",
    "\n",
    "class MessageListener(Listener):  # Event listener must inherit Listener\n",
    "    \"\"\"\n",
    "    This is the message listener. Based on the question this portion will be\n",
    "    answer. This will be responsible for conversation with user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bot):\n",
    "        self.bot = bot\n",
    "        self.m_count = 0\n",
    "\n",
    "    def on_message(self, message):  # called on every message\n",
    "        self.m_count += 1\n",
    "        print(f\"Total messages: {self.m_count}\")\n",
    "        ans = retrievalQA.invoke(message.text)\n",
    "        chunks = split_message(ans[\"result\"])\n",
    "        for chunk in chunks:\n",
    "            self.bot.send_message(message.chat.id, chunk)\n",
    "\n",
    "    def on_command_failure(self, message, err=None):  # When command fails\n",
    "        if err is None:\n",
    "            self.bot.send_message(message.chat.id, \"Command failed to bind arguments!\")\n",
    "        else:\n",
    "            self.bot.send_message(message.chat.id, f\"Error in command:\\n{err}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    token = \"PUT_TELEGRAM_TOKEN_HERE\"\n",
    "    bot = Bot(token)  # Create instance of OrigamiBot class\n",
    "\n",
    "    # Add an event listener\n",
    "    bot.add_listener(MessageListener(bot))\n",
    "\n",
    "    # Add a command holder\n",
    "    bot.add_commands(BotsCommands(bot))\n",
    "\n",
    "    # We can add as many command holders\n",
    "    # and event listeners as we like\n",
    "\n",
    "    bot.start()  # start bot's threads\n",
    "    print(\"*\" * 25)\n",
    "    print(\"Bot has been started!!!\")\n",
    "    while True:\n",
    "        sleep(1)\n",
    "        # Can also do some useful work i main thread\n",
    "        # Like autoposting to channels for example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
